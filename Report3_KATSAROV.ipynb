{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43598ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from IPython.display import Image\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "\n",
    "\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import model_selection,preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc8cd7f",
   "metadata": {},
   "source": [
    "## Step 1 - Geant4 simulation (20%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e433c6a2",
   "metadata": {},
   "source": [
    "Creating a detector that consists of:\n",
    "\n",
    "A homogenous EM calorimeter (ECAL) made from lead-glass (10cm in length)\n",
    "\n",
    "A sampling hadronic calorimeter (HCAL): 5 lead (3cm) and 5 liquid argon (30cm) layers alternating "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72886e3",
   "metadata": {},
   "source": [
    "## Step 2 - Data reconstruction (20%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d969f5",
   "metadata": {},
   "source": [
    "To detect the particle energies the ECAL and HCAL are made into sensitive detectors\n",
    "\n",
    "The simulation is ran separately for a particle gun of electrons, photons, protons and neutrons. The particle gun energy is incremented by 5 MeV for every particle being fired. This allows to obtain a range of particle energies. 5000 particles are fired for each particle type with the energy starting at 200 MeV and finishing at 25 GeV. This produces 20000 events in total.\n",
    "\n",
    "The simulation precision was slightly reduced to speed up the computation times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4aac53",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = [\"TrueEnergy\", \"ECAL\",\"HLayer1\", \"HLayer2\", \"HLayer3\", \"HLayer4\", \"HLayer5\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7931dd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = col_names[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f084725",
   "metadata": {},
   "source": [
    "### Load the particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585b9800",
   "metadata": {},
   "outputs": [],
   "source": [
    "electrons = pd.read_csv(\"Electrons2.csv\",comment=\"#\",\n",
    "                                header=None,names=col_names,dtype=\"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81c281f",
   "metadata": {},
   "outputs": [],
   "source": [
    "photons = pd.read_csv(\"Photons2.csv\",comment=\"#\",\n",
    "                                header=None,names=col_names,dtype=\"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd5f576",
   "metadata": {},
   "outputs": [],
   "source": [
    "protons = pd.read_csv(\"Protons2.csv\",comment=\"#\",\n",
    "                                header=None,names=col_names,dtype=\"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39df901f",
   "metadata": {},
   "outputs": [],
   "source": [
    "neutrons = pd.read_csv(\"Neutrons2.csv\",comment=\"#\",\n",
    "                                header=None,names=col_names,dtype=\"float\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc69d09",
   "metadata": {},
   "source": [
    "### Give labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9dc4215",
   "metadata": {},
   "source": [
    "Particle IDs\n",
    "\n",
    "electron = 0, photon = 1, proton = 2, neutron = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af213a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "electrons.insert(loc=0,column=\"ID\",value=np.zeros(electrons.shape[0]))\n",
    "photons.insert(loc=0,column=\"ID\",value=np.ones(protons.shape[0]))\n",
    "protons.insert(loc=0,column=\"ID\",value=2*np.ones(protons.shape[0]))\n",
    "neutrons.insert(loc=0,column=\"ID\",value=3*np.ones(protons.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73823cb8",
   "metadata": {},
   "source": [
    "### Combine datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21182898",
   "metadata": {},
   "outputs": [],
   "source": [
    "particles = pd.concat([electrons,photons,protons,neutrons],ignore_index=True)\n",
    "particles.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f51864",
   "metadata": {},
   "outputs": [],
   "source": [
    "particles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f52b5be",
   "metadata": {},
   "source": [
    "### Create training and testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1b8f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into features and target sets\n",
    "features = particles[particles.columns[2:]]\n",
    "target = particles.ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129b628c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standarise data and split into training and test set\n",
    "sc = preprocessing.StandardScaler()\n",
    "features = sc.fit_transform(features)\n",
    "# set random seed\n",
    "seed = 1\n",
    "# train - test split of dataset\n",
    "x_train, x_test, y_train, y_test, train_energy, test_energy = \\\n",
    "model_selection.train_test_split(features, target, particles.TrueEnergy, test_size=0.3,random_state = seed)\n",
    "print (x_train.shape, x_test.shape, y_train.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701a098a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_model(num_inputs, num_nodes, extra_depth,drop_out):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    # Input shape equal to number of features (6 detector readouts)\n",
    "    model.add(Dense(num_nodes, input_dim=num_inputs, kernel_initializer=\"normal\", activation=\"relu\"))\n",
    "    # Add dropout to prevent overtraining\n",
    "    model.add(Dropout(drop_out))\n",
    "    for i in range (extra_depth):\n",
    "        # Adding dense layers\n",
    "        model.add(Dense(num_nodes, kernel_initializer=\"normal\", activation=\"relu\"))\n",
    "        # Following each dense layer by dropout layer\n",
    "        model.add(Dropout(drop_out))\n",
    "    model.add(Dense(4, activation=\"softmax\",kernel_initializer=\"normal\"))\n",
    "    # Compile model to use multi-class loss\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\",optimizer=\"adam\", metrics =[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0744bb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model with chosen hyper parameters\n",
    "num_nodes = 50\n",
    "extra_depth = 2\n",
    "drop_out=0.2\n",
    "model = my_model(x_train.shape[1],num_nodes,extra_depth,drop_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e58c572",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b80f291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training hyperparameters\n",
    "batchSize = 50\n",
    "N_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efa4698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model and save the training history\n",
    "history = model.fit(x_train,y_train,batch_size=batchSize,epochs=N_epochs,verbose=1,\n",
    "                    validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3d53c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])\n",
    "plt.title(\"Model Loss\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend([\"loss\",\"val_loss\"])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.plot(history.history[\"accuracy\"])\n",
    "plt.plot(history.history[\"val_accuracy\"])\n",
    "plt.title(\"Model Validation Accuracy\")\n",
    "plt.ylabel(\"Validation accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend([\"acc\",\"val_acc\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162c3b0b",
   "metadata": {},
   "source": [
    "The model is training well. The validation loss is continuously falling and has the same shape as training loss, signifying that no overtraining takes place. The validation loss is below the training loss becuase we include regularisation through dropout while training. \n",
    "\n",
    "The accuracy curves show consistent improvement. The accuracy acheived is reasonably high given that we are classifying between 4 different particles where two pairs of particles have very similar features considering only the enegy deposits alone. However, the model still obtains a meaningful discriminating power through the training. It does considerably better than guessing which would give only 25% accuracy.\n",
    "\n",
    "The validation accuracy is above the training accuracy due to including regularisation during training in the form of dropout. \n",
    "\n",
    "The best val_accuracy achieved by the model is 0.5818"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b20c0c9",
   "metadata": {},
   "source": [
    "## Step 3 - Data validation (20%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1369851",
   "metadata": {},
   "source": [
    "### Testing the performance of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1565e589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the likelihood for each particle initiating each test event\n",
    "score = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5eaf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the particle ID predicted with highest likelihood\n",
    "prediction = np.argmax(model.predict(x_test),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede49d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test,prediction,normalize=\"true\")\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=[\"Electrons\",\"Photons\",\"Protons\",\"Neutrons\"])\n",
    "\n",
    "disp.plot()\n",
    "plt.title(\"Confusion matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9916dc",
   "metadata": {},
   "source": [
    "From the is confusion matrix we can see quite an interesting result.\n",
    "\n",
    "The network often finds that classifying all of the electrons and photons as photons achieves the highest accuracy.\n",
    "\n",
    "Training the model different times usually produces an alternation between predicting all electrons and photons and photons and predicting all electrons and photons as electrons. This shows that the information taken from the collisions is very similar between photons and electrons and the classifier cannot distinguish between the two. \n",
    "\n",
    "This is largely because the electrons and photons are both completely stopped by the ECAl in almost all cases. Both particle have the same energy and therefore nothing else can distinguish between them.\n",
    "\n",
    "\n",
    "In the case of the protons and neutrons, the classifier gets a very high accuracy for the protons but much lower for the neutrons. In the course of this it decides to classify many of the neutrons as protons. It looks like the classifier is more confindent in classfying protons so it prioritises classying proton-like particles as protons to achieve a higher accuracy.\n",
    "\n",
    "Through different trainings it has been found that the classification between protons and neutrons can alternate in a similar way to how it alternates for electrons and photons.\n",
    "\n",
    "Here we can see that the classifier has the most difficulty classifying between electrons/photons and protons/neutrons. This is largely because without knowing their charges these sets of particles behave in a very similar way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d1ba92",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(200,5190,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e40ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = np.histogram(test_energy,bins= np.linspace(200,5190,20),weights=((y_test==prediction)*1).values)[0]\n",
    "total = np.histogram(test_energy,bins= np.linspace(200,5190,20))[0]\n",
    "bin_width = bins[1]-bins[0]\n",
    "bin_centres = np.linspace(bins[0]+(bin_width/2),bins[-1]-(bin_width/2),19)\n",
    "plt.hist(bin_centres,bins=bins,weights=correct/total)\n",
    "plt.title(\"Accuracy as a function of energy\")\n",
    "plt.xlabel(\"Energy [MeV]\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10de0f58",
   "metadata": {},
   "source": [
    "From this plot we can see that the accuracy has a weak relationship with the energy for the data we provide.\n",
    "\n",
    "However there is a slight trend of increase in performance with energy, up to a certain point. \n",
    "\n",
    "This is because at low energies electrons, photons and most protons are stopped by the EM caloriemeter, however, at higher energies only the electrons and photons are stopped and protons and neutrons continue onwards. Therefore at higher energies the particles begin to show more different behaviour compared to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06f657b",
   "metadata": {},
   "source": [
    "### Resolution of energy measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be5a18b",
   "metadata": {},
   "source": [
    "Calculating the calibration quality for each event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a2278d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolution(ID):\n",
    "    E_true = particles[particles.ID==ID][col_names[0]]\n",
    "    E_detected = particles[particles.ID==ID][layers].values.sum(axis=1)\n",
    "    \n",
    "    # Dealing with missing energy values\n",
    "    ratio = E_true/E_detected\n",
    "    ratio=ratio.values\n",
    "    ratio[np.where(ratio==np.inf)[0]]=0\n",
    "    \n",
    "    \n",
    "    calibration = np.average(ratio)\n",
    "    E_calibrated = E_detected*calibration\n",
    "    calibration_quality = (E_calibrated-E_true)/E_true\n",
    "    return E_true, calibration, calibration_quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f8fb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the 2D Histograms\n",
    "fig,ax = plt.subplots(2,2,figsize=(12,9))\n",
    "\n",
    "e_E_true, e_calibration, e_calibration_quality = resolution(0) \n",
    "g_E_true, g_calibration, g_calibration_quality = resolution(1) \n",
    "p_E_true, p_calibration, p_calibration_quality = resolution(2) \n",
    "n_E_true, n_calibration, n_calibration_quality = resolution(3) \n",
    "\n",
    "ax[0,0].set_title(\"Calibration quality of electrons\")\n",
    "ax[0,0].hist2d( x=e_E_true, y=e_calibration_quality, bins=(10, 20) )\n",
    "ax[0,0].set_ylabel(\"($E_{cal}-E_{true})/E_{true}$\")\n",
    "ax[0,0].set_xlabel(\"True energy [MeV]\")\n",
    "\n",
    "ax[0,1].set_title(\"Calibration quality of photons\")\n",
    "ax[0,1].hist2d( x=g_E_true, y=g_calibration_quality, bins=(10, 20) )\n",
    "ax[0,1].set_ylabel(\"($E_{cal}-E_{true})/E_{true}$\")\n",
    "ax[0,1].set_xlabel(\"True energy [MeV]\")\n",
    "\n",
    "ax[1,0].set_title(\"Calibration quality of protons\")\n",
    "ax[1,0].hist2d( x=p_E_true, y=p_calibration_quality, bins=(10, 20) )\n",
    "ax[1,0].set_ylabel(\"($E_{cal}-E_{true})/E_{true}$\")\n",
    "ax[1,0].set_xlabel(\"True energy [MeV]\")\n",
    "\n",
    "ax[1,1].set_title(\"Calibration quality of neutrons\")\n",
    "ax[1,1].hist2d( x=n_E_true, y=n_calibration_quality, bins=(10, 20) )\n",
    "ax[1,1].set_ylabel(\"($E_{cal}-E_{true})/E_{true}$\")\n",
    "ax[1,1].set_xlabel(\"True energy [MeV]\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd1b3f7",
   "metadata": {},
   "source": [
    "Caribration quality is very good for electrons and photons as they get stopped completely in the ECAL. These particles also have a very good energy resolution. This can be seen through the sharp line in their respective plots.\n",
    "\n",
    "The calibration quality for protons is worse likely because they deposit a lower fraction of their energy in the detectors. The resolution is also lower as can be seen through the more smeared out line.\n",
    "\n",
    "The neutrons have the worst calibration quality as they interact the least with the detectors and deposit the least energy. This can be seen through less consistent calibration quality and hence most smeared out line, producing the lowest resolution.\n",
    "\n",
    "In some cases the neutrons leave no energy deposit in the detector so these cases need to be treated with care during calibration. This is also the likeliest reason the mean of the calibration quality is significantly shifted from zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d339bf",
   "metadata": {},
   "source": [
    "## Step 4 - Improvement and discussion (40%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02675c5",
   "metadata": {},
   "source": [
    "Improvemnts:\n",
    "\n",
    "Add silicon tracking layer (1cm) to determine if the particle entering ECAL is charged or not (better discrimination between electrons/photons and protons/neutrons)\n",
    "\n",
    "Increase lead to lAr ratio (5cm,25cm) in HCAL and increase layers to 8 (should allow larger fraction of energy to be deposited by protons and neutrons to increase energy resolution)\n",
    "\n",
    "Add a magnetic field of 0.05 Tesla in x direction so the trajectory of charged particles will be curved. This should be able to provide more information to the machine learning model from the position of the hits in the silicon detector, and whether there have been any hits at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f7b54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_col_names = [\"TrueEnergy\", \"ECAL\",\"HLayer1\", \"HLayer2\", \"HLayer3\", \"HLayer4\", \"HLayer5\",\"HLayer6\",\n",
    "             \"HLayer7\",\"HLayer8\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87255023",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_layers = new_col_names[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b7139c",
   "metadata": {},
   "source": [
    "### Load the particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f07bfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "electrons_new = pd.read_csv(\"Electrons_improved.csv\",comment=\"#\",\n",
    "                                header=None,names=new_col_names,dtype=\"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0c5986",
   "metadata": {},
   "outputs": [],
   "source": [
    "photons_new = pd.read_csv(\"Photons_improved.csv\",comment=\"#\",\n",
    "                                header=None,names=new_col_names,dtype=\"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8261cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "protons_new = pd.read_csv(\"Protons_improved.csv\",comment=\"#\",\n",
    "                                header=None,names=new_col_names,dtype=\"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f13741",
   "metadata": {},
   "outputs": [],
   "source": [
    "neutrons_new = pd.read_csv(\"Neutrons_improved.csv\",comment=\"#\",\n",
    "                                header=None,names=new_col_names,dtype=\"float\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d66abc",
   "metadata": {},
   "source": [
    "### Load the tracking information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ad2b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker_cols = [\"ID\",\"Theta\",\"Phi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d38290",
   "metadata": {},
   "outputs": [],
   "source": [
    "electrons_tracks = pd.read_csv(\"Electrons_tracker.csv\",comment=\"#\",\n",
    "                                header=None,names=tracker_cols,dtype=\"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6d1e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "photons_tracks = pd.read_csv(\"Photons_tracker.csv\",comment=\"#\",\n",
    "                                header=None,names=tracker_cols,dtype=\"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6db0661",
   "metadata": {},
   "outputs": [],
   "source": [
    "protons_tracks = pd.read_csv(\"Protons_tracker.csv\",comment=\"#\",\n",
    "                                header=None,names=tracker_cols,dtype=\"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77976ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "neutrons_tracks = pd.read_csv(\"Neutrons_tracker.csv\",comment=\"#\",\n",
    "                                header=None,names=tracker_cols,dtype=\"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5a4bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Event_IDs = np.int64(electrons_tracks.ID.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbacd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introducing new variables based on the tracking detector.\n",
    "# The new variables should allow for an improved distinction between differently charged particles \n",
    "# and charged particles of different energies and masses. \n",
    "\n",
    "def pre_process(tracks, energies, Event_IDs):\n",
    "    event_hits = pd.DataFrame(columns=[\"Event_ID\",\"N_hits\",\"Theta1\",\"Phi1\"])\n",
    "    for i in Event_IDs:\n",
    "        first_hit_theta =  tracks[tracks.ID==i].Theta.values[0]\n",
    "        first_hit_phi = tracks[tracks.ID==i].Phi.values[0]\n",
    "        n_hits = tracks[tracks.ID==i].Theta.values.size\n",
    "\n",
    "        # Saving hit data to dataframe\n",
    "        event_hits.loc[len(event_hits)]=[i,n_hits,first_hit_theta,first_hit_phi]\n",
    "    tracks_energy = pd.concat([event_hits,energies],axis=1)\n",
    "    tracks_energy.drop(\"Event_ID\",axis=1,inplace=True)\n",
    "    True_energy = tracks_energy.pop(\"TrueEnergy\")\n",
    "    tracks_energy.insert(0, True_energy.name, True_energy)\n",
    "    \n",
    "    return tracks_energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144a2893",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_track_energy = pre_process(electrons_tracks,electrons_new,Event_IDs)\n",
    "g_track_energy = pre_process(photons_tracks,photons_new,Event_IDs)\n",
    "p_track_energy = pre_process(protons_tracks,protons_new,Event_IDs)\n",
    "n_track_energy = pre_process(neutrons_tracks,neutrons_new,Event_IDs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6211530b",
   "metadata": {},
   "source": [
    "### Give labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5a7c7e",
   "metadata": {},
   "source": [
    "Particle IDs\n",
    "\n",
    "electron = 0, photon = 1, proton = 2, neutron = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c353bce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_track_energy.insert(loc=0,column=\"ID\",value=np.zeros(e_track_energy.shape[0]))\n",
    "g_track_energy.insert(loc=0,column=\"ID\",value=np.ones(g_track_energy.shape[0]))\n",
    "p_track_energy.insert(loc=0,column=\"ID\",value=2*np.ones(p_track_energy.shape[0]))\n",
    "n_track_energy.insert(loc=0,column=\"ID\",value=3*np.ones(n_track_energy.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc97661",
   "metadata": {},
   "source": [
    "### Combine datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3cde01",
   "metadata": {},
   "outputs": [],
   "source": [
    "particles_new = pd.concat([e_track_energy,g_track_energy,p_track_energy,n_track_energy],ignore_index=True)\n",
    "particles_new.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9e6506",
   "metadata": {},
   "outputs": [],
   "source": [
    "particles_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735912a3",
   "metadata": {},
   "source": [
    "### Create training and testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b14a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into features and target sets\n",
    "features = particles_new[particles_new.columns[2:]]\n",
    "target = particles_new.ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b747fc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standarise data and split into training and test set\n",
    "sc = preprocessing.StandardScaler()\n",
    "features = sc.fit_transform(features)\n",
    "# set random seed\n",
    "seed = 1\n",
    "# train - test split of dataset\n",
    "x_train, x_test, y_train, y_test, train_energy, test_energy = \\\n",
    "model_selection.train_test_split(features, target, particles.TrueEnergy, test_size=0.3,random_state = seed)\n",
    "print (x_train.shape, x_test.shape, y_train.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1baed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_model(num_inputs, num_nodes, extra_depth,drop_out):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    # Input shape equal to number of features (6 detector readouts)\n",
    "    model.add(Dense(num_nodes, input_dim=num_inputs, kernel_initializer=\"normal\", activation=\"relu\"))\n",
    "    # Add dropout to prevent overtraining\n",
    "    model.add(Dropout(drop_out))\n",
    "    for i in range (extra_depth):\n",
    "        # Adding dense layers\n",
    "        model.add(Dense(num_nodes, kernel_initializer=\"normal\", activation=\"relu\"))\n",
    "        # Following each dense layer by dropout layer\n",
    "        model.add(Dropout(drop_out))\n",
    "    model.add(Dense(4, activation=\"softmax\",kernel_initializer=\"normal\"))\n",
    "    # Compile model to use multi-class loss\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\",optimizer=\"adam\", metrics =[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f4654c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model with chosen hyper parameters\n",
    "num_nodes = 50\n",
    "extra_depth = 2\n",
    "drop_out=0.2\n",
    "model = my_model(x_train.shape[1],num_nodes,extra_depth,drop_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc9367a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee37680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training hyperparameters\n",
    "batchSize = 50\n",
    "N_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79694110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model and save the training history\n",
    "history = model.fit(x_train,y_train,batch_size=batchSize,epochs=N_epochs,verbose=1,\n",
    "                    validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20b7be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])\n",
    "plt.title(\"Model Loss\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend([\"loss\",\"val_loss\"])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.plot(history.history[\"accuracy\"])\n",
    "plt.plot(history.history[\"val_accuracy\"])\n",
    "plt.title(\"Model Validation Accuracy\")\n",
    "plt.ylabel(\"Validation accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend([\"acc\",\"val_acc\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99de244",
   "metadata": {},
   "source": [
    "The model now does much better on the new training data. All of the particle have distinct properties which can differentiate them now. Therefore the model achieves very high classification accuracy, where it can predit almost any event with complete certainty. After including the tracking information the problem to classfy the particles becomes much easier. The best val_accuracy achieved by the model is 0.9984.\n",
    "\n",
    "However, in this case the model can become too complex for the easy problem at hand and can start to overtrain if left to train for too long. This is not a problem as training can be stopped earlier and best model performance can be saved before overtraining begins. \n",
    "\n",
    "Once again the validation loss and validation accuracy are above the loss and accuracy as regularisation in the form of drop-out is used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b7af40",
   "metadata": {},
   "source": [
    "## Step 3 - Data validation (20%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882394fe",
   "metadata": {},
   "source": [
    "### Testing the performance of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9a13c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the likelihood for each particle initiating each test event\n",
    "score = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4541111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the particle ID predicted with highest likelihood\n",
    "prediction = np.argmax(model.predict(x_test),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b63121",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test,prediction,normalize=\"true\")\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=[\"Electrons\",\"Photons\",\"Protons\",\"Neutrons\"])\n",
    "\n",
    "disp.plot()\n",
    "plt.title(\"Confusion matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e103a47",
   "metadata": {},
   "source": [
    "This confusion matrix shows a much more different results compared to the previous one.\n",
    "\n",
    "We can see that across all classes there is effectively 100% classification accuracy.\n",
    "\n",
    "In a very few cases neutrons have been interpreted as photons and in even fewer cases photons have been interpreted as neutrons. This shows that there could be some possible room to improve distinction between these two particles by adding more detector components.\n",
    "\n",
    "Apart from this the current detector layout works very well for this set of particle events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0997fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(200,5190,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935837b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = np.histogram(test_energy,bins= np.linspace(200,5190,20),weights=((y_test==prediction)*1).values)[0]\n",
    "total = np.histogram(test_energy,bins= np.linspace(200,5190,20))[0]\n",
    "bin_width = bins[1]-bins[0]\n",
    "bin_centres = np.linspace(bins[0]+(bin_width/2),bins[-1]-(bin_width/2),19)\n",
    "plt.hist(bin_centres,bins=bins,weights=correct/total)\n",
    "plt.title(\"Accuracy as a function of energy\")\n",
    "plt.xlabel(\"Energy [MeV]\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "# plt.xscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9e2fd0",
   "metadata": {},
   "source": [
    "From this plot we can see a similar trend to the previous detector layout. \n",
    "\n",
    "The accuracy is visibly decreased at the lowest energies and at higher energy we achieve close to 100% accuracy. \n",
    "\n",
    "This is because at low energies electrons, photons and most protons are stopped by the EM caloriemeter, however, at higher energies only the electrons and photons are stopped and protons and neutrons continue onwards. Therefore at higher energies the particles begin to show more different behaviour compared to each other.\n",
    "\n",
    "However, there is an interesting decrease in accuracy near 3.5 GeV. This is likely near the point where neutrons experience lowest energy loss through ionisation, but this is only my guess."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0e6e54",
   "metadata": {},
   "source": [
    "### Resolution of energy measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbf2044",
   "metadata": {},
   "source": [
    "Calculating the calibration quality for each event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9f9c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolution(ID):\n",
    "    E_true = particles_new[particles_new.ID==ID][new_col_names[0]]\n",
    "    E_detected = particles_new[particles_new.ID==ID][new_layers].values.sum(axis=1)\n",
    "    \n",
    "    # Dealing with missing energy values\n",
    "    ratio = E_true/E_detected\n",
    "    ratio=ratio.values\n",
    "    ratio[np.where(ratio==np.inf)[0]]=0\n",
    "    \n",
    "    \n",
    "    calibration = np.average(ratio)\n",
    "    E_calibrated = E_detected*calibration\n",
    "    calibration_quality = (E_calibrated-E_true)/E_true\n",
    "    return E_true, calibration, calibration_quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f20449",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the 2D Histograms\n",
    "fig,ax = plt.subplots(2,2,figsize=(12,9))\n",
    "\n",
    "e_E_true, e_calibration, e_calibration_quality = resolution(0) \n",
    "g_E_true, g_calibration, g_calibration_quality = resolution(1) \n",
    "p_E_true, p_calibration, p_calibration_quality = resolution(2) \n",
    "n_E_true, n_calibration, n_calibration_quality = resolution(3) \n",
    "\n",
    "ax[0,0].set_title(\"Calibration quality of electrons\")\n",
    "ax[0,0].hist2d( x=e_E_true, y=e_calibration_quality, bins=(10, 20) )\n",
    "ax[0,0].set_ylabel(\"($E_{cal}-E_{true})/E_{true}$\")\n",
    "ax[0,0].set_xlabel(\"True energy [MeV]\")\n",
    "\n",
    "ax[0,1].set_title(\"Calibration quality of photons\")\n",
    "ax[0,1].hist2d( x=g_E_true, y=g_calibration_quality, bins=(10, 20) )\n",
    "ax[0,1].set_ylabel(\"($E_{cal}-E_{true})/E_{true}$\")\n",
    "ax[0,1].set_xlabel(\"True energy [MeV]\")\n",
    "\n",
    "ax[1,0].set_title(\"Calibration quality of protons\")\n",
    "ax[1,0].hist2d( x=p_E_true, y=p_calibration_quality, bins=(10, 20) )\n",
    "ax[1,0].set_ylabel(\"($E_{cal}-E_{true})/E_{true}$\")\n",
    "ax[1,0].set_xlabel(\"True energy [MeV]\")\n",
    "\n",
    "ax[1,1].set_title(\"Calibration quality of neutrons\")\n",
    "ax[1,1].hist2d( x=n_E_true, y=n_calibration_quality, bins=(10, 20) )\n",
    "ax[1,1].set_ylabel(\"($E_{cal}-E_{true})/E_{true}$\")\n",
    "ax[1,1].set_xlabel(\"True energy [MeV]\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc4faae",
   "metadata": {},
   "source": [
    "Caribration quality is very good for electrons and photons. However the calibration quality of the electrons now varies more with energy. This is most likely because the electrons interact differently with the silicon detector at different energies. However the calibration quality is still very consistent at each energy (high resolution) for both electrons and photons which is seen through the sharp lines.\n",
    "\n",
    "We can also see that the calibration quality for protons and neutrons has significantly improved. The calibration quality varies a lot less with energy compared to the previous detector layout. This is largely due to the increase in absorber thickness and number of HCAL layers. Furthermore, we can see that the energy resolution has also improved due to the sharper lines for both plots. \n",
    "\n",
    "We can however still see that the mean calibration quality for neutrons is shifted from zero. This indicates that a fraction neutrons still pass through the detector without depositing energy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5bcb5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
